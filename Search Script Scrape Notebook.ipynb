{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Script Scrape\n",
    "\n",
    "Search Script Scrape was created by Dan Nguyen, to learn his students at Stanford Journalism College how to scrape by writing 101 scrapers. Below you'll find the scrapers as written by me, Winny de Jong. Some scrapers are written multiple times using different libraries/techniques. Just because. \n",
    "\n",
    "All scrapers worked at a certain point in time; from scraper 009 onwards, I've added this certain point of time. :) \n",
    "\n",
    "- [The original Github Repository](https://github.com/stanfordjournalism/search-script-scrape)\n",
    "- Article by Dan Nguyen [with background info](http://blog.danwin.com/examples-of-web-scraping-in-python-3-x-for-data-journalists/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 001 - using beautifulsoup and requests  \n",
    "Goal:  get the number of datasets currently listed on [data.gov](http://www.data.gov/)\n",
    "Updated: July 21st, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:33:00.610707Z",
     "start_time": "2018-04-08T01:33:00.341153Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253,246\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('http://www.data.gov/')\n",
    "\n",
    "# make bs object from website\n",
    "soup = BeautifulSoup(site.content, 'html.parser')\n",
    "\n",
    "# save the text found in the span with class text-color-red as datasets\n",
    "datasets = soup.find('span', {'class': 'text-color-red'}).text\n",
    "\n",
    "# print text inside datasets\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 002 v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 002 - using requests and lxml and css select  \n",
    "Goal: get the name of the [most recently added dataset on data.gov](https://catalog.data.gov/dataset?q=&sort=metadata_created+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:35:41.382022Z",
     "start_time": "2018-04-08T01:35:41.228862Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H13648: NOS Hydrographic Survey , 2022-12-05\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# import site by get request in variable site1\n",
    "site = requests.get('https://catalog.data.gov/dataset?q=&sort=metadata_created+desc')\n",
    "\n",
    "# save html as text in variable doc\n",
    "doc = html.fromstring(site.text)\n",
    "\n",
    "# strip the text content of the first [0] link found by selecting css; save it as ds1\n",
    "ds1 = doc.cssselect('h3')[0].text_content().strip()\n",
    "\n",
    "# print result\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 002 v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 002 - using requests and BeautifulSoup  \n",
    "Goal: get the name of the [most recently added dataset on data.gov](https://catalog.data.gov/dataset?q=&sort=metadata_created+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:50:36.980820Z",
     "start_time": "2018-04-08T01:50:36.677906Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H13648: NOS Hydrographic Survey , 2022-12-05'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# create a variable with the url\n",
    "url = 'https://catalog.data.gov/dataset?q=&sort=metadata_created+desc'\n",
    "\n",
    "# Use requests to get the contents\n",
    "r = requests.get(url)\n",
    "\n",
    "# get the text of the contents\n",
    "html_content = r.text\n",
    "\n",
    "# convert the html content into a beautiful soup object\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# get text of first h3 heading\n",
    "soup.h3.a.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 003, using requests and navigating a json file  \n",
    "Goal: get the number of [people who visited a U.S. government website](https://analytics.usa.gov/data/live/ie.json) using Internet Explorer 6.0 in the last 90 days   \n",
    "Written april 8th, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T11:51:08.460630Z",
     "start_time": "2018-04-08T11:51:07.810674Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297116\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://analytics.usa.gov/data/live/ie.json\")\n",
    "# read json file with .json(), select info totals > ie_version > 6.0\n",
    "ie6 = (r.json()['totals']['ie_version']['6.0'])\n",
    "print(ie6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## TO DO 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T01:37:13.075795Z",
     "start_time": "2018-04-08T01:37:13.070514Z"
    },
    "hidden": true
   },
   "source": [
    "Scraper 004 - using using requests and navigating a json file  \n",
    "Goal: get the number of librarian-related job positions that the federal government is currently hiring for, code for librarian-related positions is 1410.  \n",
    "Written July 21st, 2023.\n",
    "\n",
    "The [API has changed](https://developer.usajobs.gov/API-Reference/), you need to [request an authorization key](https://developer.usajobs.gov/APIRequest/) first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'<html>\\r\\n<head>\\r\\n<title>USAJOBS</title>\\r\\n\\r\\n <style>\\r\\n\\t* { font-fa'\n",
      " b'mily: verdana; font-size: 10pt; COLOR: gray; }\\r\\n\\tb { font-weight: bold; '\n",
      " b'}\\r\\n\\ttable { height: 50%; border: 1px solid gray;}\\r\\n\\ttd { text-align:'\n",
      " b' center; padding: 10;}\\r\\n\\t</style>\\r\\n</head>\\r\\n<body> \\r\\n<center>\\r\\n<'\n",
      " b'br><br><br><br>\\r\\n\\t<table>\\r\\n\\t<tr><td><img src=\"/apology_files/usajobs'\n",
      " b'-seal.jpg\" /> </td></tr>\\r\\n\\t<!--<tr><td><img src=\"P:\\\\RSS\\\\USAJ\\\\Akamai\\\\'\n",
      " b'New\\\\Regular\\\\apology_files\\\\usajobs-seal.jpg\" /> </td></tr>-->\\r\\n\\t<tr><'\n",
      " b'td  style=\"font-size: 12pt\">\\r\\n                    <span class=\"style1\">W'\n",
      " b'e\\x92re sorry for the inconvenience, but there appears to be a slight probl'\n",
      " b'em bringing up the page you requested.</span><br>\\r\\n                    <'\n",
      " b'span class=\"style1\">We\\x92re working on it and will have it back up soon. <'\n",
      " b'/span>\\r\\n                    <span class=\"style1\">Thank you for  your pat'\n",
      " b'ience \\x96 the USAJOBS team. </span><br />\\r\\n                    <br /><'\n",
      " b'br/> \\r\\n                    <p><font color=\"#003A5D\">Reference # <span>10'\n",
      " b'2.d51fc917.1689947671.133afff</span></font></p>\\r\\n\\t</td></tr>\\r\\n</tabl'\n",
      " b'e>\\r\\n<br><br>\\r\\n</center>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n</body>\\r\\n</html>\\r\\n')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "headers = {'Host': 'data.usajobs.gov',\n",
    "           'User-Agent': '',\n",
    "           'Authorization-Key': ''\n",
    "          }\n",
    "\n",
    "params = {'JobCategoryCode': '1410'\n",
    "         }\n",
    "\n",
    "r = requests.get('https://data.usajobs.gov/api/Search', \n",
    "                  params=params,\n",
    "                  headers=headers)\n",
    "\n",
    "pprint(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TO DO 005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 005, using ...  \n",
    "Goal: get the name of the company cited in the most recent consumer complaint involving student loans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 006, using requests and beautiful soup  \n",
    "Goal: get the the change in median cost of health, dental, and vision coverage for California city employees from 2010 to 2013.\n",
    "\n",
    "Since the provided URL gave me errors, I opted for [transparent California](https://transparentcalifornia.com/salaries/2016/california-city/) as an alternative. I'm looking up the median total benefits for employees of California City from 2011 to 2014.\n",
    "  \n",
    "Written december 24th 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T20:41:30.143713Z",
     "start_time": "2018-06-27T20:41:27.203934Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>29887.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>29887.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>29887.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>29887.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year    median\n",
       "0  2011  29887.17\n",
       "1  2012  29887.17\n",
       "2  2013  29887.17\n",
       "3  2014  29887.17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://transparentcalifornia.com/salaries/XXX/california-city/\"\n",
    "data = []\n",
    "\n",
    "for i in range(2011,2015):\n",
    "    i = str(i)\n",
    "    url = url.replace(\"XXX\", i)\n",
    "    r = requests.get(url)\n",
    "    # get the text of the contents\n",
    "    html_content = r.text\n",
    "    # convert the html content into a beautiful soup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # get the div that contains the pagination\n",
    "    divPages = soup.find('div', {'class': 'pagination pagination-centered'})\n",
    "    # find all list items of the pagination\n",
    "    pages = divPages.find_all('li')\n",
    "    # since the pagination ends with 'Next >', the second last \n",
    "    # item contains the highest pagenumber\n",
    "    maxPage = len(pages) - 2\n",
    "    \n",
    "    # for every page in pagerange:\n",
    "    for page in range(0, maxPage):\n",
    "        # request page...\n",
    "        url = url + '?page=' + str(page)\n",
    "        r = requests.get(url)\n",
    "        # get the text of the contents\n",
    "        html_content = r.text\n",
    "        # convert the html content into a beautiful soup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # get all rows froms table\n",
    "        for t in soup.find_all('tr'):\n",
    "            cells = t.find_all('td')\n",
    "            try:\n",
    "                year = i\n",
    "                # get name from first td\n",
    "                name = cells[0].text.strip()\n",
    "                # get benefits from 7th td, replace $ and ,\n",
    "                benefits = cells[6].text.replace('$', '').replace(',', '')\n",
    "                # append information to list with data\n",
    "                data.append([year, name, benefits])\n",
    "            except:\n",
    "                # this exception is necessary since some tr's do not contain any\n",
    "                # cells, but instead are used to show an ad-banner. We want to simply\n",
    "                # pass those banner containing tr's...\n",
    "                pass\n",
    "    \n",
    "# NOTE: get out of for-loops = no more indentation\n",
    "\n",
    "# create a dataframe based upon data\n",
    "df = pd.DataFrame(data)\n",
    "# set column names\n",
    "df.columns = ['year', 'name', 'benefits']\n",
    "# create empty list to later collect medians in\n",
    "medians = []\n",
    "# calculate median benefits for every year\n",
    "for year in df['year'].unique():\n",
    "    # create sub dataframe\n",
    "    subDF = df[df['year'] == year]\n",
    "    subDF\n",
    "    # calculate median benefits for subDF\n",
    "    median = subDF['benefits'].median()\n",
    "    # append data to list of medians\n",
    "    medians.append([year, median])\n",
    "\n",
    "# create dataframe based on list of lists\n",
    "medianDF = pd.DataFrame(medians)\n",
    "# rename columns\n",
    "medianDF.columns = ['year', 'median']\n",
    "# return dataframe medianDF\n",
    "medianDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## x 007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The [URL provided](https://inventory.data.gov/dataset/federal-executive-agency-internet-domains-as-of-06172014/resource/b626ef1f-9019-41c4-91aa-5ae3f7457328) is no longer valid, alternative URL's require a login.  \n",
    "Goal: get the number of listed federal executive agency internet domains "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 008 v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 008 using requests and pandas and a url that leads to a CSV-file.  \n",
    "Goal: get the number of times when a New York heart surgeon's rate of patient deaths for all cardiac surgical procedures was \"significantly higher\" than the statewide rate, according to New York state's analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-28T11:54:18.246Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://health.data.ny.gov/resource/msvb-mnxf.csv\"\n",
    "data = pd.read_csv(url)\n",
    "records = data.loc[data['comparison_results'] == 'Rate significantly higher than Statewide Rate']\n",
    "print(len(records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 008, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 008 using requests and a url that leads to a JSON-file.   \n",
    "Goal: get the number of times when a New York heart surgeon's rate of patient deaths for all cardiac surgical procedures was \"significantly higher\" than the statewide rate, according to New York state's analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-28T11:54:04.851Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://health.data.ny.gov/resource/dk4z-k3xb.json'\n",
    "string = 'Rate significantly higher than Statewide Rate'\n",
    "data = requests.get(url).json()\n",
    "records = [r for r in data if string in r['comparison_results']]\n",
    "print(len(records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper 009, using the requests and [xmltodict](https://github.com/martinblech/xmltodict) modules    \n",
    "Goal: get the number of [roll call votes](https://www.senate.gov/legislative/LIS/roll_call_lists/vote_menu_114_1.htm) that were rejected by a margin of less than 5 votes, in the first session of the U.S. Senate in the 114th Congress.    \n",
    "Written: July 21st, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T21:12:32.056628Z",
     "start_time": "2018-06-27T21:12:31.529426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "from pprint import pprint\n",
    "\n",
    "# url of xml with all data\n",
    "url = \"https://www.senate.gov/legislative/LIS/roll_call_lists/vote_menu_114_1.xml\"\n",
    "\n",
    "# request xml\n",
    "r = requests.get(url)\n",
    "\n",
    "# make a dict called data out of xml data\n",
    "data = xmltodict.parse(r.content)\n",
    "\n",
    "\n",
    "# I used pretty print (pprint) to get a feel of what the XML looks like\n",
    "#pprint(data)\n",
    "\n",
    "# set count to zero\n",
    "count = 0\n",
    "\n",
    "# the XML contains a vote summary > which contains votes > which contains vote \n",
    "# we need to iterate over the issues saved in vote\n",
    "# for i in vote in votes in vote_summary in data:\n",
    "for i in data['vote_summary']['votes']['vote']:\n",
    "    # if the result of i is Rejected:\n",
    "    if i['result'] == 'Rejected':\n",
    "        # get the nays from the vote_tally of i, make it into an integer\n",
    "        nays = int(i['vote_tally']['nays'])\n",
    "        # get the yeas from the vote_tally of i, make it into an integer\n",
    "        yeas = int(i['vote_tally']['yeas'])\n",
    "        \n",
    "        # if nays minus yeas is 5 or less\n",
    "        if nays - yeas <= 5:\n",
    "            # add one to count\n",
    "            count =+ 1\n",
    "        # if nays minus yeas is more than 5 pass\n",
    "        else:\n",
    "            pass\n",
    "    # if result of i is Accepted (not Rejected) pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# print count\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper 010, using requests and beautiful soup.  \n",
    "Goal: get the title of the highest paid California city government position in 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chief Administrative Officer\n",
      "Bell\n",
      "Administration\n",
      "$765,237\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# a Google search gave me this url\n",
    "url = 'https://publicpay.ca.gov/Reports/Cities/Cities.aspx?year=2010&rpt=1'\n",
    "# a look at the source code of that url, showed me that the actual request\n",
    "# takes place elsewhere, namely:\n",
    "dataUrl = 'https://publicpay.ca.gov/Ajax/Reports/GetRptTopEmployees.aspx?year=2010&entitytypeid=1&rptfilter=0&showfilter=True'\n",
    "# request website\n",
    "r = requests.get(dataUrl)\n",
    "# create soup\n",
    "soup = BeautifulSoup(r.content)\n",
    "# get all jobs from table\n",
    "jobs = soup.find_all('tr')\n",
    "\n",
    "# get highestPaid looking nicely\n",
    "# get all td's from first row in jobs\n",
    "# note: I'm using [1] since [0] contains headers\n",
    "for td in jobs[1].find_all('td')[1:]:\n",
    "    # print stripped text of every td \n",
    "    print(td.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 011 using requests, json and BeautifulSoup.\n",
    "Goal: find out how much did the state of California collect in property taxes, according to the U.S. Census 2013 Annual Survey of State Government Tax Collections?\n",
    "\n",
    "*Notes*  \n",
    "If you look at [this page](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk), the network tab in Chrome developer tools shows that the data comes from another url, see below.\n",
    "  \n",
    "Scraper written December 27th, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# set headers: copy from my browser\n",
    "headers = {'Accept': '*/*',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'nl-NL,nl;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Cookie': 'ak_bmsc=F141B031F58896A0556D9BC25A06B6711743FBB7EB4C00002BF8035EE4547F4E~plOfABtjiiW6tc86p/D+M3clisI2pBwS0MDroFF/A+vKjl0uqKh2LuoGS0t0/9TJ/xAQZzn0ghZd5ek4sanPRczK9TK/zH3I7mMzrV3Rq9ISBsSCJAguan+NiyUT0Q5wFGBRery8468yqn7Aughnzz7wI9SkPdHCne+z8WU1sUUmJ+Bb86EYQXozmC5bl2hu03RnRe7UleaGX9Y3y9JoQWPNcXKXgoamytzsX7UkKjc0w=; _ga=GA1.2.1296164313.1577318444; _gid=GA1.2.1812031464.1577318444; s_fid=26D55BF9E0FDB2E5-30BB5F650193B29B; s_getNewRepeat=1577318444088-New; undefined_s=First%20Visit; s_vnum=1582502444099%26vn%3D1; s_invisit=true; s_cc=true; bm_sv=327AF40BF9F873E4BD3486AC06677B88~LnNsp6HKl9IDCQ4wtITBQO4w2eCLZezeRaZ0Y4cke7IqYN4BY/tRn1yK25J+A03i/kikQxBrrHDxWKKbiUtlMEJZuUF1ZfTObS5fx9wdIlskP8KKLJZoD1stSflUVg1YyC7DPUE49L2yex9H+euboN9qb09GMrXFlUPvb7j0lDg=; JSESSIONID=0001Psr7hoaOmtp3CbuBwf2RtK-:17vl840fa; TS01fa88a5=011ba694f2450b38d6b789354b5cff313038224357cf4ce9d92cee4cc8c28b964ec6b0cc1f4ca7697ad57e0add2a0dd76041c6399e; RDS_DADS_PERSIST=!ScS2W5qxyJqY5tGkG9EXQU5bLFTlrkNJZEWxNlqvpw+RIuzLVKHOa/vF0WzMFylv+6oxuqxotYzLt1g=; s_gnr=1577318524900-New; s_sq=cebufactfinderprod%252Ccebucensusglobalprod%3D%2526c.%2526a.%2526activitymap.%2526page%253DAmerican%252520FactFinder%252520-%252520Results%2526link%253DModify%252520Table%2526region%253DtableToolText%2526pageIDType%253D1%2526.activitymap%2526.a%2526.c%2526pid%253DAmerican%252520FactFinder%252520-%252520Results%2526pidt%253D1%2526oid%253Djavascript%25253AtoggleTT%252528%252529%2526ot%253DA',\n",
    "            'DNT': '1',\n",
    "            'Host': 'factfinder.census.gov',\n",
    "            'Referer': 'https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-origin',\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "            'X-Requested-With': 'XMLHttpRequest',\n",
    "            'X-Requested-With': 'XMLHttpRequest'}\n",
    "\n",
    "# set url as found in network tab\n",
    "url = 'https://factfinder.census.gov/tablerestful/tableServices/renderProductData?renderForMap=f&renderForChart=f&pid=STC_2013_00A2&prodToReplace=STC_2015_00A2&log=t&_ts=595364512240'\n",
    "# do request\n",
    "r = requests.get(url, headers=headers)\n",
    "# load json\n",
    "data = json.loads(r.text)\n",
    "# select table\n",
    "table = data['ProductData']['productDataTable']\n",
    "# create soup: json table contains html table\n",
    "soup = BeautifulSoup(table, 'html.parser')\n",
    "# select propertyTaxes from soup\n",
    "# skip some elements - td's are used for spacing -_-\n",
    "propertyTaxes = soup.find('th', {'id': 'r4'}\n",
    "                         ).next_element.next_element.next_element.next_element.text\n",
    "# print result\n",
    "print(propertyTaxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scraper 012 using tabula-py and pandas; taking a shortcut by [parsing a PDF](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/media/cy10_primary_enplanements.pdf). :)   \n",
    "Goal: for 2010, get the year-over-year change in enplanements at America's busiest airport  \n",
    "\n",
    "Scraper written at December 27th, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tabula import read_pdf\n",
    "import pandas as pd\n",
    "\n",
    "# where to find this PDF?\n",
    "urlPDF = 'https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/media/cy10_primary_enplanements.pdf'\n",
    "# read pdf, create dataframe\n",
    "df = read_pdf(urlPDF, pages='all')\n",
    "# rename columns (printed df first time to check column headers)\n",
    "df.rename(columns = {'Unnamed: 0':'Rank',\n",
    "                     'CY 10\\rEnplanements': 'CY 10 Enplanements',\n",
    "                     'CY 09\\rEnplanements': 'CY 09 Enplanements',\n",
    "                     '%\\rChange': '% Change'}, inplace=True)\n",
    "# sort df descending by value in CY 10 Enplanements column\n",
    "df.sort_values(by='CY 10 Enplanements', ascending=False)\n",
    "# since order is descending for CY 10 Enplanements column, \n",
    "# the biggest airport is in the first row = [0]; \n",
    "# to find the change, I make ints so I can subtract, 2010 minus 2009.\n",
    "int(df.iloc[0]['CY 10 Enplanements'].replace(',', '')) - int(df.iloc[0]['CY 09 Enplanements'].replace(',',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TO DO 013\n",
    "??Scraper 013 using tabula-py and pandas. [PDF with bank crime statistics](https://www.fbi.gov/file-repository/stats-services-publications-bank-crime-statistics-2014-bank-crime-statistics-2014) via [this page](https://www.fbi.gov/file-repository/stats-services-publications-bank-crime-statistics-2014-bank-crime-statistics-2014/view).    \n",
    "\n",
    "Goal: get the number of armored carrier bank robberies recorded by the FBI in 2014.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper 014 using requests, beautifulsoup and pandas.  \n",
    "Goal: get the number of workplace fatalities at reported to the federal and state OSHA in the latest fiscal year.\n",
    "\n",
    "Written December 27th, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# set base url\n",
    "base = 'https://www.osha.gov'\n",
    "# found archive url\n",
    "archive = 'https://www.osha.gov/fatalities/reports/archive'\n",
    "# get archive, make soup, search for links\n",
    "r = requests.get(archive)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# create empty list called links\n",
    "links = []\n",
    "# for every link found in the soup\n",
    "for a in soup.find_all('a'):\n",
    "    try:\n",
    "        # see if the href of said link contains 'csv'\n",
    "        if 'csv' in a['href']:\n",
    "            # if so, append to list\n",
    "            links.append(a['href'])\n",
    "        # if not, pass\n",
    "        else:\n",
    "            pass\n",
    "    # if no href found, also pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# get latest csv = first item from list\n",
    "csvUrl = base + links[0]\n",
    "df = pd.read_csv(csvUrl)\n",
    "# print the length (no of rows) of the dataframe, when the dataframe \n",
    "# only contains rows where Fatality or Catastrophe column == 'Fatality'\n",
    "print(len(df[df['Fatality or Catastrophe '] == 'Fatality']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 15 - 94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 095\n",
    "Goal: since 2002, the most commonly occurring winning number in New York's Lottery Mega Millions.  \n",
    "Using: requests and counter.  \n",
    "Written December 27th, 2019.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# create counter\n",
    "counter = Counter()\n",
    "\n",
    "# parse json data\n",
    "numbers = requests.get('https://data.ny.gov/resource/5xaw-6ayf.json').json()\n",
    "\n",
    "# for every number, split + add to counter\n",
    "for n in numbers:\n",
    "    counter.update(n['winning_numbers'].split(' '))\n",
    "\n",
    "# print most common number + number of times it appeared in dataset\n",
    "print(counter.most_common()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 096\n",
    "Goal: get the number of scheduled arguments according to the most recent U.S. Supreme Court argument calendar.  \n",
    "Using: requests, beautifulsoup, datetime, and regex.  \n",
    "Written: December 27th, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "today = datetime.today()\n",
    "\n",
    "# request url\n",
    "url = 'https://www.supremecourt.gov/oral_arguments/calendarsandlists.aspx'\n",
    "r = requests.get(url)\n",
    "# make soup, select all links to pdf called argument_calendar\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "pdfs = soup.find_all('a', href=re.compile('argument_calendar'))\n",
    "# create empty list of sessions\n",
    "sessions = []\n",
    "# for every pdf in pdfs\n",
    "for p in pdfs:\n",
    "    # go to parent, and select the text\n",
    "    session = p.parent.text.strip().replace('Session Beginning ','')\n",
    "    # clean text up some more\n",
    "    ses = re.search('(^.*[0-9]{4})\\s+\\(', session).group(1)\n",
    "    # use datetime to convert strings to datetime objects\n",
    "    sesDate = datetime.strptime(ses, '%B %d, %Y')\n",
    "    # check if sesDate is after today\n",
    "    if sesDate > today:\n",
    "        # if so, add sesDate to list sessions\n",
    "        sessions.append(sesDate)\n",
    "    # if not, pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# print length of sessions list = planned sessions in the future\n",
    "print(len(sessions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 097\n",
    "Goal: The New York school with the highest rate of religious exemptions to vaccinations\n",
    "Using: requests, operator.  \n",
    "Written: December 27th, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from operator import itemgetter\n",
    "\n",
    "url = 'https://health.data.ny.gov/resource/5pme-xbs5.json'\n",
    "data = requests.get(url).json()\n",
    "# create subset: only if period contains 2019\n",
    "p1819 = [r for r in data if '2019' in r['report_period']]\n",
    "# from subset p1819, get all percentreligiousexemptions\n",
    "# print schoolname of school with max percentreligiousexemptions\n",
    "max(p1819, key=itemgetter('percentreligiousexemptions'))['schoolname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: sort p1819 by percentreligiousexemptions in reverse order,\n",
    "# print schoolname of first row =  [0]['schoolname']\n",
    "sorted(p1819, key = lambda i: i['percentreligiousexemptions'],reverse=True)[0]['schoolname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 098\n",
    "Using: beautifulsoup en requests.  \n",
    "Goal: get pct change population in Detroit, Michigan.  \n",
    "Written: December 27th, 2019.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# go to census url for Detroit\n",
    "url = 'https://www.census.gov/quickfacts/fact/table/detroitcitymichigan,MI/PST045218'\n",
    "# get page\n",
    "r = requests.get(url)\n",
    "# create soup\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# find population percent change \n",
    "findPct = soup.find('span', string='Population, percent change - April 1, 2010 (estimates base) to July 1, 2018,  (V2018)')\n",
    "# get percentage\n",
    "pct = findPct.next_element.next_element.next_element.next_element.text.strip()\n",
    "print(pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 099"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 100\n",
    "Using:   \n",
    "Goals: get the California city whose city manager earns the most total wage per population of its city in 2012.   \n",
    "Written December 27th, 2019.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 101\n",
    "Goal: get the number of women currently serving in the U.S. Congress. Since the Sunlight Foundation is no longer running an API, I opted for the govtrack.us site.   \n",
    "Using: requests + json.  \n",
    "Written December 27th, 2019.   \n",
    "\n",
    "*Note:  \n",
    "I found the exact location of the number of females,   \n",
    "by looking into the json-file using pprint.   \n",
    "Here's what I did:  \n",
    "Step 1: pprint.pprint(data)  \n",
    "Step 2: locate the info you want, in this case it's in 'options'  \n",
    "Step 3: pprint.pprint(data['options'])  \n",
    "Step 4: locate the info you want, in this case it's in '6'  \n",
    "Step 5: pprint.pprint(data['options'][6])  \n",
    "... etcetera*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# url found while looking at network tab in browser\n",
    "url = 'https://www.govtrack.us/congress/members/current?sort=sortname&page=1&faceting=false&allow_redirect=false&do_search=1'\n",
    "# do requests\n",
    "r = requests.get(url)\n",
    "# load json\n",
    "data = json.loads(r.text)\n",
    "\n",
    "# number of females in congress\n",
    "data['options'][6][2][2][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
